{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2692405b-8e06-4a03-8e39-87752f0e7ccf",
   "metadata": {},
   "source": [
    "https://drlee.io/build-an-nlp-model-for-sentiment-analysis-using-tensorflow-in-10-minutes-a6d3de84b17f"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4866584e-18eb-483d-a619-57a1629204c3",
   "metadata": {},
   "source": [
    "1. Carregando os dados de importados e tratados do BACEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b204ba-0bc9-417c-bb23-88db67a95d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Texto</th>\n",
       "      <th>Selic</th>\n",
       "      <th>IPCA</th>\n",
       "      <th>Selic (6m)</th>\n",
       "      <th>Sentimento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2002-02-20</td>\n",
       "      <td>sumário atividade econômica ambiente externo p...</td>\n",
       "      <td>19.00</td>\n",
       "      <td>7.51</td>\n",
       "      <td>18.50</td>\n",
       "      <td>dovish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2001-11-21</td>\n",
       "      <td>sumário atividade econômica ambiente externo p...</td>\n",
       "      <td>19.00</td>\n",
       "      <td>7.61</td>\n",
       "      <td>18.75</td>\n",
       "      <td>dovish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>2009-10-21</td>\n",
       "      <td>sumário evolução recente da economia avaliação...</td>\n",
       "      <td>8.75</td>\n",
       "      <td>4.17</td>\n",
       "      <td>8.75</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>2024-09-18</td>\n",
       "      <td>a atualização da conjuntura econômica e do cen...</td>\n",
       "      <td>10.50</td>\n",
       "      <td>4.42</td>\n",
       "      <td>13.25</td>\n",
       "      <td>hawkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>sumário atividade econômica ambiente externo p...</td>\n",
       "      <td>25.50</td>\n",
       "      <td>15.85</td>\n",
       "      <td>26.50</td>\n",
       "      <td>hawkish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Data                                              Texto  Selic  \\\n",
       "32  2002-02-20  sumário atividade econômica ambiente externo p...  19.00   \n",
       "29  2001-11-21  sumário atividade econômica ambiente externo p...  19.00   \n",
       "110 2009-10-21  sumário evolução recente da economia avaliação...   8.75   \n",
       "229 2024-09-18  a atualização da conjuntura econômica e do cen...  10.50   \n",
       "45  2003-02-19  sumário atividade econômica ambiente externo p...  25.50   \n",
       "\n",
       "      IPCA  Selic (6m) Sentimento  \n",
       "32    7.51       18.50     dovish  \n",
       "29    7.61       18.75     dovish  \n",
       "110   4.17        8.75    neutral  \n",
       "229   4.42       13.25    hawkish  \n",
       "45   15.85       26.50    hawkish  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "input_data = pd.read_pickle('../dados/df_ata_sentimento.pkl') \n",
    "#input_data = pd.read_json('../dados/df_ata_sentimento.json', orient='records', lines=True)\n",
    "#input_data['Data'] = pd.to_datetime(input_data['Data']).dt.date\n",
    "#input_data.tail()\n",
    "# A obtenção de dados da coluna Selic 6m dixa lacunas, limpar os dados:\n",
    "input_data = input_data.dropna(subset=['Sentimento','Texto'])\n",
    "input_data = input_data[~(input_data['Texto'].str.strip().isna() | (input_data['Texto'].str.strip() == ''))]\n",
    "input_data['Texto'] = input_data['Texto'].str.lower()\n",
    "\n",
    "input_data.sample(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6af8d23b-498f-4972-aa07-a711a8880ac7",
   "metadata": {},
   "source": [
    "2. Separando os dados de texto da ata e sentimento para o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6393318-ce7c-4a88-ae03-643f8a6e9ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Texto</th>\n",
       "      <th>Sentimento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>sumário evolução recente da economia avaliação...</td>\n",
       "      <td>dovish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>sumário evolução recente da inflação avaliação...</td>\n",
       "      <td>dovish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>sumário atividade econômica preços ambiente ex...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>sumário evolução recente da economia avaliação...</td>\n",
       "      <td>dovish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>sumário atividade econômica ambiente externo p...</td>\n",
       "      <td>dovish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Texto Sentimento\n",
       "103  sumário evolução recente da economia avaliação...     dovish\n",
       "59   sumário evolução recente da inflação avaliação...     dovish\n",
       "25   sumário atividade econômica preços ambiente ex...    neutral\n",
       "81   sumário evolução recente da economia avaliação...     dovish\n",
       "34   sumário atividade econômica ambiente externo p...     dovish"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_data = input_data[['Texto','Sentimento']]\n",
    "base_data.sample(10)\n",
    "filename_root = 'sentimento_6m_pretrained_'\n",
    "subset_dir = '../../subsets/'\n",
    "basepickle = subset_dir + filename_root + '__base.pkl'\n",
    "base_data.to_pickle(basepickle)\n",
    "pd.read_pickle(basepickle).sample(5)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c70f52e4-6858-47c1-907a-7159d5932437",
   "metadata": {},
   "source": [
    "3. Preparando os rótulos para processamento. Produzindo dois tipos de rótulos para estudo, um hot encode con três clunad binárias para hdovish, awkish e neutral. Uma quarta coluna com um rótulo de 3 valores, 0,1 e 2\n",
    "0 = dovish\n",
    "1 = hawkisk\n",
    "2 = neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43bc8b0d-4886-4354-8717-42ac46c7f5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text     sent  sent_encoded  \\\n",
      "93   sumário evolução recente da economia avaliação...   dovish             0   \n",
      "32   sumário atividade econômica ambiente externo p...   dovish             0   \n",
      "210  a atualização da conjuntura econômica e do cen...  hawkish             1   \n",
      "4    sumário demanda e oferta agregadas ambiente ex...  neutral             2   \n",
      "199  a atualização da conjuntura econômica e do cen...  hawkish             1   \n",
      "\n",
      "     sent_dovish  sent_hawkish  sent_neutral  \n",
      "93          True         False         False  \n",
      "32          True         False         False  \n",
      "210        False          True         False  \n",
      "4          False         False          True  \n",
      "199        False          True         False  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "#tratamento dos dados para modelo\n",
    "training_data = base_data.copy()\n",
    "#Encoding do label alfanumérico\n",
    "label_encoder = LabelEncoder()\n",
    "training_data.loc[:, 'sent_encoded'] = label_encoder.fit_transform(training_data['Sentimento'])\n",
    "\n",
    "\n",
    "# Criando 3 colnuas com hot encoding\n",
    "one_hot = pd.get_dummies(training_data['Sentimento'], prefix='sent')\n",
    "training_data = pd.concat([training_data, one_hot], axis=1)\n",
    "\n",
    "# Renomeando colunas\n",
    "training_data = training_data.rename(columns={'Texto': 'text'})\n",
    "training_data = training_data.rename(columns={'Sentimento': 'sent'})\n",
    "\n",
    "# Display the result\n",
    "print(training_data.sample(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8913c914-df68-4ca4-b7c6-3d73e7ee5da5",
   "metadata": {},
   "source": [
    "4. Calculando o número total de palavras para tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91cca182-22a4-4ea7-8eb5-c2195a689f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Com pretrino não será usada tokenização.\n",
    "# Concatenando todas as atas\n",
    "\n",
    "#all_text = ' '.join(training_data['text'].tolist())\n",
    "\n",
    "#words = pd.Series(all_text.split())\n",
    "\n",
    "# Contando as palavras únicas\n",
    "\n",
    "#word_count = words.nunique()\n",
    "\n",
    "#print(f\"Palavras únicas na base: {word_count}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b93ad943-ecac-4530-aecd-b437f3fa96ab",
   "metadata": {},
   "source": [
    "5. Variáveis globais - Escopo do modelo e dimensões\n",
    "Ainda não geramos um modelo com label con três valores, \n",
    "por enquanto modelando cada sentimento sepaeradamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48f51760-c6a4-4d8c-a3ed-ecb41b59cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Switch de qual label vai ser estudado.\n",
    "label_set = 'sent_encoded'\n",
    "#label_set = 'sent_dovish'\n",
    "#label_set = 'sent_hawkish'\n",
    "#label_set = 'sent_neutral'\n",
    "\n",
    "#Globais de processsamento\n",
    "rnd_seed = 42\n",
    "num_epochs = 50\n",
    "num_batches = 16\n",
    "token_count = 30000\n",
    "padding_size = 80000\n",
    "val_split = 0.2\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44c2109d-1eaa-4228-80b4-7917f8d99716",
   "metadata": {},
   "source": [
    "6. Preenchimento (padding) e tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0904049-51da-4abb-802b-08746c72c30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Não será usado token nem pad para pre embed\n",
    "#from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenizando para 30000 palavras, o dobro do medido na base\n",
    "#tokenizer = Tokenizer(num_words=token_count, oov_token='<OOV>')\n",
    "#tokenizer.fit_on_texts(training_data['text'])\n",
    "\n",
    "# Save the tokenizer to a file\n",
    "\n",
    "#token_filename = subset_dir + filename_root + '__tokenizer.pkl'\n",
    "\n",
    "#with open(token_filename, 'wb') as file:\n",
    "#    pickle.dump(tokenizer, file)\n",
    "\n",
    "# Preenchendo com 80000 caracterse, englobando o tamanho de todas as atas com alguma folga\n",
    "\n",
    "#sequences = tokenizer.texts_to_sequences(training_data['text'])\n",
    "#padded_sequences = pad_sequences(sequences, maxlen=padding_size)\n",
    "\n",
    "#print(padded_sequences)\n",
    "\n",
    "#print(\"{:.2f}\".format(sys.getsizeof(padded_sequences)/(1024*1024)),'MB')\n",
    "#print(padded_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f1b040-1979-44d7-ba10-2db7e295b891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "f070af06-2db8-49c3-8550-b97a8e3b5d36",
   "metadata": {},
   "source": [
    "6. Separando a base em conjuntos de dados X e y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efe29b28-c529-4958-ba0a-6c3b4be8d213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sumário preços e nível de atividade agregados monetários e crédito finanças públicas balanço de pa\n",
      "[0 0 0 0 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Splitting the data into features (X) and labels (y)\n",
    "X = training_data['text'].values\n",
    "y = training_data[label_set].values\n",
    "\n",
    "print(str(X[:5])[:100])\n",
    "print(str(y[:5])[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d6d8a61-3e5d-4196-a421-f92c1057f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvando X e y completos para eventual uso\n",
    "X_filename = subset_dir + filename_root + '_' + label_set + '_X_PRETRAINED_full.npy'\n",
    "np.save(X_filename, X)\n",
    "\n",
    "y_filename = subset_dir + filename_root + '_' + label_set + '_y_PRETRAINED_full.npy'\n",
    "np.save(y_filename, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a20a8d5e-58a5-4c3e-87fd-3325f3da9fd3",
   "metadata": {},
   "source": [
    "7. Dividindo a base em conjunto de treino e de teste\n",
    "a base será dividida em 80% treino e 20% teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21ea447a-c526-474e-bba9-2b17be0dae73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sumário evolução recente da economia avaliação prospectiva das tendências da inflação implementaçã\n",
      "[1 0 1 1 1]\n",
      "['sumário demanda e oferta agregadas ambiente externo preços avaliação prospectiva das tendências da\n",
      "[0 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rnd_seed)\n",
    "\n",
    "#Salvando conjuntos de treino e teste para uso posterior\n",
    "\n",
    "X_test_filename = subset_dir + filename_root +  '_' + label_set + '_X_PRETRAINED_test.npy'\n",
    "np.save(X_test_filename, X_test)\n",
    "X_train_filename = subset_dir + filename_root +  '_' + label_set + '_X_PRETRAINED_train.npy'\n",
    "np.save(X_train_filename, X_train)\n",
    "\n",
    "y_test_filename = subset_dir + filename_root + '_' + label_set + '_y_PRETRAINED_test.npy'\n",
    "np.save(y_test_filename, y_test)\n",
    "y_train_filename = subset_dir + filename_root + '_' + label_set + '_y_PRETRAINED_train.npy'\n",
    "np.save(y_train_filename, y_train)\n",
    "\n",
    "print(str(X_train[:5])[:100])\n",
    "print(y_train[:5])\n",
    "print(str(X_test[:5])[:100])\n",
    "\n",
    "print(y_test[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e96134e-e5aa-4077-afff-40f66ef8936d",
   "metadata": {},
   "source": [
    "5. Building the Neural Network with TensorFlow\n",
    "We’ll create a simple neural network with an Embedding layer, followed by two LSTM layers, and a Dense output layer.\n",
    "\n",
    "Model Explanation\n",
    "\n",
    "Embedding Layer: Converts word indices into dense vectors of fixed size (16 dimensions).\n",
    "                                                                         \n",
    "LSTM Layers: These layers capture patterns in the text over sequences of words.b\n",
    "Dense Layer: Reduces the dimensionality of the features.\n",
    "    \n",
    "Output Layer: Uses the sigmoid activation function to predict the probability of being positive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236c0bac-051f-4156-912a-d5b61791e9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf2.20/lib/python3.12/site-packages/tensorflow_hub/__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "# Define a custom Keras layer to wrap the USE layer\n",
    "class USELayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(USELayer, self).__init__(**kwargs)\n",
    "        self.use_layer = hub.KerasLayer(\n",
    "            \"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "            input_shape=[],\n",
    "            dtype=tf.string,\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.use_layer(inputs)\n",
    "\n",
    "# Define the model using Functional API\n",
    "inputs = tf.keras.Input(shape=(), dtype=tf.string)  # Input layer for strings\n",
    "x = USELayer()(inputs)  # Apply custom USE layer\n",
    "x = tf.keras.layers.Reshape((-1, 512))(x)  # Reshape to (batch_size, 1, 512) for LSTM\n",
    "x = tf.keras.layers.LSTM(64, return_sequences=True)(x)\n",
    "x = tf.keras.layers.LSTM(32)(x)\n",
    "x = tf.keras.layers.Dense(24, activation='relu')(x)\n",
    "outputs = tf.keras.layers.Dense(3, activation='softmax')(x)  # 3 classes\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Define checkpoint callback\n",
    "ckptname = '../../' + filename_root + '_' + label_set + '_' + str(num_epochs) + '_CHKP_drlee_ata.keras'\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    ckptname,\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',  # For integer labels (0, 1, 2)\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=num_epochs, validation_split=val_split, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7df6b12-2929-4ef9-b101-a2b211dd8cd4",
   "metadata": {},
   "source": [
    "import keras\n",
    "model_filename = '../../' + filename_root +  '_' + label_set+'_' + str(num_epochs) + 'PRETRAINED_drlee_ata.keras'\n",
    "print(model_filename)\n",
    "keras.saving.save_model(model, model_filename)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96ae7a51-f0de-4351-a22c-bd69c8a25136",
   "metadata": {},
   "source": [
    "6. Visualizing Model Performance\n",
    "We can use Matplotlib to plot the training and validation accuracy over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef68a0a9-57bd-4340-88ea-ce75ad7907c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f53852d4-43cb-4536-942b-11d3fd89ac26",
   "metadata": {},
   "source": [
    "7. Evaluating the Model\n",
    "Let’s evaluate our model on the test data to see how well it generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7074c74-d43f-4cb4-b176-143b1f54e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62b26f1f-16d6-4a8c-95f5-d5ef59e1e5a5",
   "metadata": {},
   "source": [
    "8. Making Predictions\n",
    "We can use our model to predict the sentiment of new movie reviews."
   ]
  },
  {
   "cell_type": "raw",
   "id": "af013e28-f680-4ce7-8a1f-c7de9600ce7e",
   "metadata": {},
   "source": [
    "\n",
    "sample_reviews = [\n",
    "    \"I absolutely loved this movie! The plot was thrilling and the characters were so well developed.\",\n",
    "    \"The film was a disaster. Poor acting and a predictable storyline.\"\n",
    "]\n",
    "\n",
    "sample_sequences = tokenizer.texts_to_sequences(sample_reviews)\n",
    "sample_padded = pad_sequences(sample_sequences, maxlen=200)\n",
    "\n",
    "predictions = model.predict(sample_padded)\n",
    "print([\"Positive\" if prob > 0.5 else \"Negative\" for prob in predictions])\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
