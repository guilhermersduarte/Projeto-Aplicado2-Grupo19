{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2692405b-8e06-4a03-8e39-87752f0e7ccf",
   "metadata": {},
   "source": [
    "https://drlee.io/build-an-nlp-model-for-sentiment-analysis-using-tensorflow-in-10-minutes-a6d3de84b17f"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4866584e-18eb-483d-a619-57a1629204c3",
   "metadata": {},
   "source": [
    "1. Carregando os dados de importados e tratados do BACEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b204ba-0bc9-417c-bb23-88db67a95d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Texto</th>\n",
       "      <th>Selic</th>\n",
       "      <th>IPCA</th>\n",
       "      <th>Selic (6m)</th>\n",
       "      <th>Sentimento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>2007-07-18</td>\n",
       "      <td>sumário evolução recente da economia avaliação...</td>\n",
       "      <td>12.00</td>\n",
       "      <td>3.74</td>\n",
       "      <td>11.25</td>\n",
       "      <td>dovish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2008-07-23</td>\n",
       "      <td>sumário evolução recente da economia avaliação...</td>\n",
       "      <td>12.25</td>\n",
       "      <td>6.37</td>\n",
       "      <td>13.75</td>\n",
       "      <td>hawkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>2014-10-29</td>\n",
       "      <td>sumário evolução recente da economia avaliação...</td>\n",
       "      <td>11.00</td>\n",
       "      <td>6.59</td>\n",
       "      <td>12.75</td>\n",
       "      <td>hawkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2010-09-01</td>\n",
       "      <td>sumário evolução recente da economia avaliação...</td>\n",
       "      <td>10.75</td>\n",
       "      <td>4.49</td>\n",
       "      <td>11.25</td>\n",
       "      <td>hawkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2001-10-17</td>\n",
       "      <td>sumário atividade econômica ambiente externo p...</td>\n",
       "      <td>19.00</td>\n",
       "      <td>7.19</td>\n",
       "      <td>19.00</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Data                                              Texto  Selic  \\\n",
       "92  2007-07-18  sumário evolução recente da economia avaliação...  12.00   \n",
       "100 2008-07-23  sumário evolução recente da economia avaliação...  12.25   \n",
       "150 2014-10-29  sumário evolução recente da economia avaliação...  11.00   \n",
       "117 2010-09-01  sumário evolução recente da economia avaliação...  10.75   \n",
       "28  2001-10-17  sumário atividade econômica ambiente externo p...  19.00   \n",
       "\n",
       "     IPCA  Selic (6m) Sentimento  \n",
       "92   3.74       11.25     dovish  \n",
       "100  6.37       13.75    hawkish  \n",
       "150  6.59       12.75    hawkish  \n",
       "117  4.49       11.25    hawkish  \n",
       "28   7.19       19.00    neutral  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "input_data = pd.read_pickle('../dados/df_ata_sentimento.pkl') \n",
    "#input_data = pd.read_json('../dados/df_ata_sentimento.json', orient='records', lines=True)\n",
    "#input_data['Data'] = pd.to_datetime(input_data['Data']).dt.date\n",
    "#input_data.tail()\n",
    "# A obtenção de dados da coluna Selic 6m dixa lacunas, limpar os dados:\n",
    "input_data = input_data.dropna(subset=['Sentimento','Texto'])\n",
    "input_data = input_data[~(input_data['Texto'].str.strip().isna() | (input_data['Texto'].str.strip() == ''))]\n",
    "input_data['Texto'] = input_data['Texto'].str.lower()\n",
    "\n",
    "input_data.sample(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6af8d23b-498f-4972-aa07-a711a8880ac7",
   "metadata": {},
   "source": [
    "2. Separando os dados de texto da ata e sentimento para o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6393318-ce7c-4a88-ae03-643f8a6e9ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Texto</th>\n",
       "      <th>Sentimento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>sumário evolução recente da economia avaliação...</td>\n",
       "      <td>hawkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>sumário atividade econômica ambiente externo p...</td>\n",
       "      <td>hawkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>a atualização da conjuntura econômica e do cen...</td>\n",
       "      <td>hawkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>sumário evolução recente da economia avaliação...</td>\n",
       "      <td>hawkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>sumário evolução recente da inflação avaliação...</td>\n",
       "      <td>hawkish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Texto Sentimento\n",
       "150  sumário evolução recente da economia avaliação...    hawkish\n",
       "42   sumário atividade econômica ambiente externo p...    hawkish\n",
       "233  a atualização da conjuntura econômica e do cen...    hawkish\n",
       "146  sumário evolução recente da economia avaliação...    hawkish\n",
       "64   sumário evolução recente da inflação avaliação...    hawkish"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_data = input_data[['Texto','Sentimento']]\n",
    "base_data.sample(10)\n",
    "filename_root = 'sentimento_6m'\n",
    "subset_dir = '../../subsets/'\n",
    "basepickle = subset_dir + filename_root + '__base.pkl'\n",
    "base_data.to_pickle(basepickle)\n",
    "pd.read_pickle(basepickle).sample(5)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c70f52e4-6858-47c1-907a-7159d5932437",
   "metadata": {},
   "source": [
    "3. Preparando os rótulos para processamento. Produzindo dois tipos de rótulos para estudo, um hot encode con três clunad binárias para hdovish, awkish e neutral. Uma quarta coluna com um rótulo de 3 valores, 0,1 e 2\n",
    "0 = dovish\n",
    "1 = hawkisk\n",
    "2 = neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43bc8b0d-4886-4354-8717-42ac46c7f5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text     sent  sent_encoded  \\\n",
      "131  sumário evolução recente da economia avaliação...   dovish             0   \n",
      "135  sumário evolução recente da economia avaliação...  hawkish             1   \n",
      "122  sumário evolução recente da economia avaliação...  hawkish             1   \n",
      "72   sumário evolução recente da inflação avaliação...  hawkish             1   \n",
      "93   sumário evolução recente da economia avaliação...   dovish             0   \n",
      "\n",
      "     sent_dovish  sent_hawkish  sent_neutral  \n",
      "131         True         False         False  \n",
      "135        False          True         False  \n",
      "122        False          True         False  \n",
      "72         False          True         False  \n",
      "93          True         False         False  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "#tratamento dos dados para modelo\n",
    "training_data = base_data.copy()\n",
    "#Encoding do label alfanumérico\n",
    "label_encoder = LabelEncoder()\n",
    "training_data.loc[:, 'sent_encoded'] = label_encoder.fit_transform(training_data['Sentimento'])\n",
    "\n",
    "\n",
    "# Criando 3 colnuas com hot encoding\n",
    "one_hot = pd.get_dummies(training_data['Sentimento'], prefix='sent')\n",
    "training_data = pd.concat([training_data, one_hot], axis=1)\n",
    "\n",
    "# Renomeando colunas\n",
    "training_data = training_data.rename(columns={'Texto': 'text'})\n",
    "training_data = training_data.rename(columns={'Sentimento': 'sent'})\n",
    "\n",
    "# Display the result\n",
    "print(training_data.sample(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8913c914-df68-4ca4-b7c6-3d73e7ee5da5",
   "metadata": {},
   "source": [
    "4. Calculando o número total de palavras para tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91cca182-22a4-4ea7-8eb5-c2195a689f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavras únicas na base: 16738\n"
     ]
    }
   ],
   "source": [
    "# Concatenando todas as atas\n",
    "\n",
    "all_text = ' '.join(training_data['text'].tolist())\n",
    "\n",
    "words = pd.Series(all_text.split())\n",
    "\n",
    "# Contando as palavras únicas\n",
    "\n",
    "word_count = words.nunique()\n",
    "\n",
    "print(f\"Palavras únicas na base: {word_count}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b93ad943-ecac-4530-aecd-b437f3fa96ab",
   "metadata": {},
   "source": [
    "5. Variáveis globais - Escopo do modelo e dimensões\n",
    "Ainda não geramos um modelo com label con três valores, \n",
    "por enquanto modelando cada sentimento sepaeradamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48f51760-c6a4-4d8c-a3ed-ecb41b59cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Switch de qual label vai ser estudado.\n",
    "label_set = 'sent_encoded'\n",
    "#label_set = 'sent_dovish'\n",
    "#label_set = 'sent_hawkish'\n",
    "#label_set = 'sent_neutral'\n",
    "\n",
    "#Globais de processsamento\n",
    "rnd_seed = 42\n",
    "num_epochs = 150\n",
    "num_batches = 16\n",
    "token_count = 30000\n",
    "padding_size = 80000\n",
    "val_split = 0.2\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44c2109d-1eaa-4228-80b4-7917f8d99716",
   "metadata": {},
   "source": [
    "6. Preenchimento (padding) e tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0904049-51da-4abb-802b-08746c72c30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ... 2975    7   31]\n",
      " [   0    0    0 ... 2065    7   31]\n",
      " [   0    0    0 ... 2975    7   31]\n",
      " ...\n",
      " [   0    0    0 ...   10    4   16]\n",
      " [   0    0    0 ...   10    4   16]\n",
      " [   0    0    0 ...   10    4   16]]\n",
      "62.87 MB\n",
      "[   0    0    0 ... 2975    7   31]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenizando para 30000 palavras, o dobro do medido na base\n",
    "tokenizer = Tokenizer(num_words=token_count, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(training_data['text'])\n",
    "\n",
    "# Save the tokenizer to a file\n",
    "\n",
    "token_filename = subset_dir + filename_root + '__tokenizer.pkl'\n",
    "\n",
    "with open(token_filename, 'wb') as file:\n",
    "    pickle.dump(tokenizer, file)\n",
    "\n",
    "# Preenchendo com 80000 caracterse, englobando o tamanho de todas as atas com alguma folga\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(training_data['text'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=padding_size)\n",
    "\n",
    "print(padded_sequences)\n",
    "\n",
    "print(\"{:.2f}\".format(sys.getsizeof(padded_sequences)/(1024*1024)),'MB')\n",
    "print(padded_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f1b040-1979-44d7-ba10-2db7e295b891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "f070af06-2db8-49c3-8550-b97a8e3b5d36",
   "metadata": {},
   "source": [
    "6. Separando a base em conjuntos de dados X e y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efe29b28-c529-4958-ba0a-6c3b4be8d213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ... 2975    7   31]\n",
      " [   0    0    0 ... 2065    7   31]\n",
      " [   0    0    0 ... 2975    7   31]\n",
      " [   0    0    0 ... 2975    7   31]\n",
      " [   0    0    0 ... 2975    7   31]]\n",
      "[0 0 0 0 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Splitting the data into features (X) and labels (y)\n",
    "X = padded_sequences\n",
    "y = training_data[label_set].values\n",
    "\n",
    "print(X[:5])\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d6d8a61-3e5d-4196-a421-f92c1057f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvando X e y completos para eventual uso\n",
    "X_filename = subset_dir + filename_root + '_' + label_set + '_X_full.npy'\n",
    "np.save(X_filename, X)\n",
    "\n",
    "y_filename = subset_dir + filename_root + '_' + label_set + '_y_full.npy'\n",
    "np.save(y_filename, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a20a8d5e-58a5-4c3e-87fd-3325f3da9fd3",
   "metadata": {},
   "source": [
    "7. Dividindo a base em conjunto de treino e de teste\n",
    "a base será dividida em 80% treino e 20% teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21ea447a-c526-474e-bba9-2b17be0dae73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   25  244  118]\n",
      " [   0    0    0 ...  213    5  229]\n",
      " [   0    0    0 ... 6293  401   93]\n",
      " [   0    0    0 ...    7  244  118]\n",
      " [   0    0    0 ...    7   86   64]]\n",
      "[[   0    0    0 ...   31    3 3260]\n",
      " [   0    0    0 ... 2975    7   31]\n",
      " [   0    0    0 ...   10    4   16]\n",
      " [   0    0    0 ... 2192    5 2134]\n",
      " [   0    0    0 ...    8  949  316]]\n",
      "[1 0 1 1 1]\n",
      "[0 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rnd_seed)\n",
    "\n",
    "#Salvando conjuntos de treino e teste para uso posterior\n",
    "\n",
    "X_test_filename = subset_dir + filename_root +  '_' + label_set + '_X_test.npy'\n",
    "np.save(X_test_filename, X_test)\n",
    "X_train_filename = subset_dir + filename_root +  '_' + label_set + '_X_train.npy'\n",
    "np.save(X_train_filename, X_train)\n",
    "\n",
    "y_test_filename = subset_dir + filename_root + '_' + label_set + '_y_test.npy'\n",
    "np.save(y_test_filename, y_test)\n",
    "y_train_filename = subset_dir + filename_root + '_' + label_set + '_y_train.npy'\n",
    "np.save(y_train_filename, y_train)\n",
    "\n",
    "print(X_train[:5])\n",
    "print(X_test[:5])\n",
    "print(y_train[:5])\n",
    "print(y_test[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e96134e-e5aa-4077-afff-40f66ef8936d",
   "metadata": {},
   "source": [
    "5. Building the Neural Network with TensorFlow\n",
    "We’ll create a simple neural network with an Embedding layer, followed by two LSTM layers, and a Dense output layer.\n",
    "\n",
    "Model Explanation\n",
    "\n",
    "Embedding Layer: Converts word indices into dense vectors of fixed size (16 dimensions).\n",
    "                                                                         \n",
    "LSTM Layers: These layers capture patterns in the text over sequences of words.b\n",
    "Dense Layer: Reduces the dimensionality of the features.\n",
    "    \n",
    "Output Layer: Uses the sigmoid activation function to predict the probability of being positive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c0087d-c26e-41d2-bfa3-4b98085e124a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    }
   ],
   "source": [
    "# Para sent_encoded com 3 valores, 0, 1 e 2.\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(token_count, 16),\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')  # 3 classes\n",
    "])\n",
    "\n",
    "ckptname = '../../' + filename_root +  '_' + label_set+ '_' + str(num_epochs) + '_CHKP_drlee_ata.keras'\n",
    "# Define callback to save the best model based on validation loss\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    ckptname,\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',  # For integer labels (0, 1, 2)\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "#history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=32,callbacks=[checkpoint_cb])\n",
    "history = model.fit(X_train, y_train, epochs=num_epochs, validation_split=val_split,callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "295f265f-db87-4050-912a-30bac6ff8253",
   "metadata": {},
   "source": [
    "# Para sent_dovish, sent_hawkish ou sent_neutral exclusivamente 1 binário.\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(token_count, 16),\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "#ckptname = '../../drle_sent_encoded.keras' \n",
    "# Define callback to save the best model based on validation loss\n",
    "#checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "#    ckptname,\n",
    "#    monitor=\"val_loss\",\n",
    "#    save_best_only=True,\n",
    "#    save_weights_only=False\n",
    "#)\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=num_epochs, validation_split=val_split, callbacks=[checkpoint_cb])\n",
    "\n",
    "##### Fazer callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a88e783-e50f-482d-a36a-d007cfa961ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "model_filename = '../../' + filename_root +  '_' + label_set+'_' + str(num_epochs) + '_drlee_ata.keras'\n",
    "print(model_filename)\n",
    "keras.saving.save_model(model, model_filename)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96ae7a51-f0de-4351-a22c-bd69c8a25136",
   "metadata": {},
   "source": [
    "6. Visualizing Model Performance\n",
    "We can use Matplotlib to plot the training and validation accuracy over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef68a0a9-57bd-4340-88ea-ce75ad7907c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f53852d4-43cb-4536-942b-11d3fd89ac26",
   "metadata": {},
   "source": [
    "7. Evaluating the Model\n",
    "Let’s evaluate our model on the test data to see how well it generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7074c74-d43f-4cb4-b176-143b1f54e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62b26f1f-16d6-4a8c-95f5-d5ef59e1e5a5",
   "metadata": {},
   "source": [
    "8. Making Predictions\n",
    "We can use our model to predict the sentiment of new movie reviews."
   ]
  },
  {
   "cell_type": "raw",
   "id": "af013e28-f680-4ce7-8a1f-c7de9600ce7e",
   "metadata": {},
   "source": [
    "\n",
    "sample_reviews = [\n",
    "    \"I absolutely loved this movie! The plot was thrilling and the characters were so well developed.\",\n",
    "    \"The film was a disaster. Poor acting and a predictable storyline.\"\n",
    "]\n",
    "\n",
    "sample_sequences = tokenizer.texts_to_sequences(sample_reviews)\n",
    "sample_padded = pad_sequences(sample_sequences, maxlen=200)\n",
    "\n",
    "predictions = model.predict(sample_padded)\n",
    "print([\"Positive\" if prob > 0.5 else \"Negative\" for prob in predictions])\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
